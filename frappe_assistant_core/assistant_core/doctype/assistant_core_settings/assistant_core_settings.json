{
 "actions": [],
 "creation": "2025-06-21 22:45:07.030425",
 "doctype": "DocType",
 "document_type": "Setup",
 "editable_grid": 1,
 "engine": "InnoDB",
 "field_order": [
  "server_enabled",
  "plugins_section",
  "enabled_plugins_list",
  "plugin_status_html",
  "streaming_section",
  "enforce_artifact_streaming",
  "response_limit_prevention",
  "streaming_behavior_instructions",
  "streaming_line_threshold",
  "streaming_char_threshold"
 ],
 "fields": [
  {
   "default": "1",
   "description": "Enable or disable the Frappe Assistant Core MCP server",
   "fieldname": "server_enabled",
   "fieldtype": "Check",
   "label": "Enable Assistant Core"
  },
  {
   "fieldname": "plugins_section",
   "fieldtype": "Section Break",
   "label": "Plugin Configuration"
  },
  {
   "default": "[]",
   "fieldname": "enabled_plugins_list",
   "fieldtype": "JSON",
   "hidden": 1,
   "label": "Enabled Plugins"
  },
  {
   "fieldname": "plugin_status_html",
   "fieldtype": "HTML",
   "label": "Plugin Status",
   "options": "<div id=\"plugin-status-container\">Loading plugin information...</div>"
  },
  {
   "collapsible": 1,
   "fieldname": "streaming_section",
   "fieldtype": "Section Break",
   "label": "Artifact Streaming Configuration"
  },
  {
   "default": "1",
   "description": "Require all LLMs to use artifact streaming for analysis operations",
   "fieldname": "enforce_artifact_streaming",
   "fieldtype": "Check",
   "label": "Enforce Artifact Streaming Protocol"
  },
  {
   "default": "1",
   "description": "Automatically promote artifact streaming to prevent LLM response limits",
   "fieldname": "response_limit_prevention",
   "fieldtype": "Check",
   "label": "Enable Response Limit Prevention"
  },
  {
   "default": "Always create analysis workspace artifacts before performing data analysis.\nStream all detailed work to artifacts to prevent response limits.\nKeep responses minimal with artifact references.\nBuild unlimited analysis depth via progressive artifact updates.",
   "description": "Additional behavioral instructions for LLMs using the MCP server",
   "fieldname": "streaming_behavior_instructions",
   "fieldtype": "Small Text",
   "label": "Custom Streaming Instructions"
  },
  {
   "default": "5",
   "description": "Number of lines in tool result that triggers artifact streaming",
   "fieldname": "streaming_line_threshold",
   "fieldtype": "Int",
   "label": "Line Threshold for Streaming"
  },
  {
   "default": "1000",
   "description": "Number of characters in tool result that triggers artifact streaming",
   "fieldname": "streaming_char_threshold",
   "fieldtype": "Int",
   "label": "Character Threshold for Streaming"
  }
 ],
 "issingle": 1,
 "links": [],
 "modified": "2025-08-12 17:50:12.690254",
 "modified_by": "Administrator",
 "module": "Assistant Core",
 "name": "Assistant Core Settings",
 "owner": "Administrator",
 "permissions": [
  {
   "create": 1,
   "delete": 1,
   "email": 1,
   "print": 1,
   "read": 1,
   "role": "System Manager",
   "share": 1,
   "write": 1
  }
 ],
 "sort_field": "modified",
 "sort_order": "DESC",
 "states": [],
 "track_changes": 1
}